# 概率论与数理统计

## 基本概念

### 样本和概率分布

- 概率分布：是一个函数，它描述了一个随机变量所有可能取值的概率。对于离散随机变量，概率分布通常用概率质量函数（PMF）表示；对于连续随机变量，概率分布通常用概率密度函数（PDF）表示。

- 样本：是从一个概率分布中抽取的独立同分布（i.i.d.）的观测值。例如，如果我们从一个正态分布中抽取100个观测值，这100个观测值就构成了一个样本。

- **抽样过程**：样本是通过从概率分布中随机抽取得到的。抽样过程假设每次抽取都是独立的，并且每个观测值都遵循相同的概率分布。
    
- **样本的统计量**：通过对样本进行统计分析，可以估计概率分布的参数。例如，样本均值可以用来估计总体均值，样本方差可以用来估计总体方差。
    
- **大数定律**：根据大数定律，随着样本量的增加，样本均值会趋近于总体均值，样本方差会趋近于总体方差。这表明，**通过足够大的样本，我们可以很好地估计概率分布的特性。**

- **中心极限定律**：描述了样本均值的分布随着样本量的增加而趋近于正态分布的现象。具体来说，如果从任意一个具有有限均值和方差的概率分布中独立同分布地抽取样本，那么样本均值的标准化形式（减去总体均值并除以标准差）的分布会随着样本量的增加而趋近于标准正态分布。

## 霍夫丁不等式 

该不等式给出了随机变量的和与其期望值偏差的概率上限。

## KL散度 | 相对熵

KL散度（Kullback-Leibler Divergence），又称相对熵，是一种衡量两个概率分布差异的度量。它量化了一个概率分布 \( Q \) 与参考概率分布 \( P \) 之间的“距离”。KL散度的原理基于信息论，具体计算公式为：

$$ D_{KL}(P \parallel Q) = \sum_{i} P(i) \log \left( \frac{P(i)}{Q(i)} \right) $$

或

$$ D_{KL}(P \parallel Q) = \int_{-\infty}^{\infty} p(x) \log \left( \frac{p(x)}{q(x)} \right) dx $$

其中，\( P \) 和 \( Q \) 分别是两个概率分布，\( p(x) \) 和 \( q(x) \) 是它们的概率密度函数。KL散度非负，且在 \( P \) 和 \( Q \) 完全相同时为0。KL散度不对称，即 $D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P)$ 。


