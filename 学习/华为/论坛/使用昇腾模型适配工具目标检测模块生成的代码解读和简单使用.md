## 1. 背景

&emsp;开发者可以使用模型适配工具免编程地[快速构建一个典型的推理应用（点击跳转官方文档）](https://www.hiascend.com/document/detail/zh/Atlas200IDKA2DeveloperKit/23.0.RC1/Getting%20Started%20with%20Application%20Development/iaqd/iaqd_0001.html)，包括**分类模型**、**目标检测模型**、**分割模型**和**关键点模型**等。本文针对目标检测模型，解读在 `Atlas 200I DK A2` 开发板进行模型转换后，得到的推理程序，并且如何进行简单地自定义使用。

&emsp;开发者应当已经执行到以下步骤，并能成功推理回显
```
python3 om_infer.py --model=output/yolov5s_bs1.om --eval --visible
```

## 2. `om_infer.py` 代码解读

```python
def read_class_names(ground_truth_json):  
    with open(ground_truth_json, 'r') as file:  
        content = file.read()  
    content = json.loads(content)  
    categories = content.get('categories')  
    names = {}  
    for id, category in enumerate(categories):  
        category_name = category.get('name')  
        if len(category_name.split()) == 2:  
            temp = category_name.split()  
            category_name = temp[0] + '_' + temp[1]  
        names[id] = category_name.strip('\n')  
    return names
```

这个函数主要从 JSON 文件中提取出所有目标检测中的类别。若不改变 JSON 文件的格式，没有必要修改。

```python
# 在img0上绘制边界框和detect信息  
def draw_bbox(bbox, img0, color, wt, names):  
    det_result_str = ''  
    # enumerate获取每个类别的索引idx和值class_is，bbox[:, 5]表示获取bbox所有行的第六列  
    for idx, class_id in enumerate(bbox[:, 5]):  
        # bbox[idx][4]表示的是置信度  
        if float(bbox[idx][4] < float(0.05)):  
            continue  
        # 在img0上矩形框，分别确定左上坐标和右下坐标，然后确定颜色color，线宽wt  
        img0 = cv2.rectangle(img0, (int(bbox[idx][0]), int(bbox[idx][1])), (int(bbox[idx][2]), int(bbox[idx][3])),  
                             color, wt)  
        """在img0上绘制检测出来的对象名称，名字由传入的names[]确定，位置在左上角的点稍下移的位置  
        字体为cv2.FONT_HERSHEY_SIMPLEX，字号0.5，颜色(0, 0, 255)，线宽为1  
        注意，这里的色彩空间是BGR"""  
        img0 = cv2.putText(img0, str(idx) + ' ' + names[int(class_id)], (int(bbox[idx][0]), int(bbox[idx][1] + 16)),  
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)  
        # {}表示占位符，.4f表示四位小数的浮点数，:表示说明符  
        # 位置在文字下侧  
        img0 = cv2.putText(img0, '{:.4f}'.format(bbox[idx][4]), (int(bbox[idx][0]), int(bbox[idx][1] + 32)),  
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)  
        # 这段字符串没有使用，识别结果名称、置信度、左上坐标、右下坐标  
        det_result_str += '{} {} {} {} {} {}\n'.format(  
            names[bbox[idx][5]], str(bbox[idx][4]), bbox[idx][0], bbox[idx][1], bbox[idx][2], bbox[idx][3])  
    return img0
```

在传入图像上绘制边界框和检测结果。这个函数内部遍历了 `bbox[][]` ，也就是推理得到的框图，然后调用 `cv2.rectangle()` 和 `cv2.putText()` 两个函数，将方框和文本信息画到图像上。

`bbox[i][]` 中记录了第 i 个检测对象的信息。`bbox[i][0-3]` 分别为左上、右下点在输出图片中的坐标，`bbox[i][4]` 为检测目标的置信系数，`bbox[i][5]` 为检测目标的名称。

```python
# 预处理，色彩空间转换等  
def preprocess_img_batch(img_batch):  
    # BGR to RGB, HWC to CHW  
    # [..., ::-1]表示对最内层的列表逆序取值，把BGR转换成了RGB  
    # .transpose()把HWC转化成了CHW  
    img_batch = img_batch[..., ::-1].transpose(0, 3, 1, 2)  
    # 归一化为 0~1    img_batch = img_batch / 255.0  
    # 将图像数据转换为连续的float16类型数组，为什么？  
    img_batch = np.ascontiguousarray(img_batch).astype(np.float16)  
    return img_batch
```

这个函数主要将图片进行了颜色空间的转换、形状的变换和归一化处理，便于神经网络的处理。没有必要修改。

```python
def parse_args():  
    parser = argparse.ArgumentParser(description='YoloV5 offline model inference.')  
    parser.add_argument('--ground_truth_json', type=str, default="test/test.json",  
                        help='annotation file path')  
    parser.add_argument('--img-path', type=str, default="test/images", help='input images dir')  
    parser.add_argument('--model', type=str, default="output/yolov5s.om", help='om model path')  
    parser.add_argument('--batch-size', type=int, default=1, help='om batch size')  
    parser.add_argument('--device-id', type=int, default=0, help='device id')  
    parser.add_argument('--output-dir', type=str, default='output', help='output path')  
    parser.add_argument('--eval', action='store_true', help='compute mAP')  
    parser.add_argument('--visible', action='store_true',  
                        help='draw detect result at image and save to output/img')  
    args = parser.parse_args()  
    return args
```

这个函数定义了命令行中可以解析的参数，可以根据需要自己添加。

## 3. 使用的例子

```python
parser.add_argument('--camera-id', type=int, default=0, help='camera id')  
# 置信度设置  
parser.add_argument('--conf', type=float, default=0.25, help='conf of detector')   
parser.add_argument('--cnt', type=int, default=25, help='the number to trigger the alarm')
```

可以在 `parse_args()` 中添加有用的命令行参数，比如设置置信度 `conf` ，或是摄像机编号 `camera_id` 。注意，在这里需要使用短横线 `-` 来代替下划线 `__` ，但在命令行中仍然使用下划线 `__`。

```python
for j, id in enumerate(pred_all[:, 5]):  
    if int(id) == 0:  
        cnt += 1  
for pred, bbox in zip(pred_all.tolist(), box.tolist()):  
    det_result_list.append({'image_id': image_id,  
                            'category_id': category_ids[int(pred[5])],  
                            'bbox': [round(x, 3) for x in bbox],  
                            'score': round(pred[4], 5)})
```

可以在主函数的这个部分添加功能，比如这里统计了 id 编号为 0 的检测对象在图像中的个数，可以在主函数中对该结果进行利用。