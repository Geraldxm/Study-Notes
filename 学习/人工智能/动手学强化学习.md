[初探强化学习 (boyuai.com)](https://hrl.boyuai.com/chapter/1/%E5%88%9D%E6%8E%A2%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/)

# 1. 初步了解
## 1.2 什么是强化学习

- 强化学习是机器通过**与环境交互**来实现目标的一种计算方法。
	- 机器和环境的一轮交互是指，机器在环境的一个状态下做一个动作决策，把这个动作作用到环境当中，这个环境发生相应的改变并且将**相应的奖励反馈和下一轮状态**传回机器。
	- 这种交互是迭代进行的，机器的目标是最大化在多轮交互过程中**获得的累积奖励的期望**。
- 强化学习用**智能体（agent）** 这个概念来表示做决策的机器。
	- 相比于有监督学习中的“模型”，强化学习中的“智能体”强调机器不但可以感知周围的环境信息，还可以通过做决策来直接改变这个环境，而不只是给出一些预测信号。
	- 智能体的三个关键因素：**感知、决策、奖励**
- 和监督学习的区别
	- 强化学习有多轮决策，即序贯决策；而预测任务通常是单轮任务
	- 智能体每轮做决策时考虑未来环境相应的改变，所以当前轮带来最大奖励反馈的动作，在长期来看并不一定是最优的。


## 1.3 强化学习的环境

$$
环境下一状态 \sim P(\cdot | 环境当前状态，智能体的动作) 
$$

与面向决策任务的智能体进行交互的环境是一个动态的随机过程，其未来状态的分布由当前状态和智能体决策的动作来共同决定，并且每一轮状态转移都伴随着两方面的随机性：一是智能体决策的动作的随机性，二是环境基于当前状态和智能体动作来采样下一刻状态的随机性。通过对环境的动态随机过程的刻画，我们能清楚地感受到，在动态随机过程中学习和在一个固定的数据分布下学习是非常不同的。


## 1.4 强化学习的目标

强化学习和有监督学习的学习目标其实是一致的，即在某个数据分布下优化一个分数值的期望。不过，经过后面的分析我们会发现，强化学习和有监督学习的优化途径是不同的。

智能体和环境每次进行交互时，环境会产生相应的奖励信号，其往往由实数标量来表示。这个奖励信号一般是诠释当前状态或动作的好坏的及时反馈信号。

整个交互过程的每一轮获得的奖励信号可以进行累加，形成智能体的整体回报（return）。


## 1.5 强化学习中的数据

监督学习中，任务建立在从给定的数据分布中采样得到的训练数据集上，通过优化在训练数据集中设定的目标函数（如最小化预测误差）来找到模型的最优参数。这里，训练数据集背后的数据分布是完全不变的。

强化学习中，**数据是在智能体与环境交互的过程中得到的**。如果智能体不采取某个决策动作，那么该动作对应的数据就永远无法被观测到，所以当前智能体的训练数据来自之前智能体的决策结果。因此，智能体的策略不同，与环境交互所产生的数据分布就不同。

**占用度量（occupancy measure）**: 归一化的占用度量用于衡量在一个智能体决策与一个动态环境的交互过程中，采样到一个具体的状态动作对（state-action pair）的概率分布。

- 强化学习的策略在训练中会不断更新，其对应的数据分布（即占用度量）也会相应地改变。
- 由于奖励建立在状态动作对之上，一个策略对应的价值其实就是一个占用度量下对应的奖励的期望，因此寻找最优策略对应着寻找最优占用度量。

## 1.6 强化学习与监督学习

监督学习：
$$\text { 最优模型 }=\arg \min _{\text {模型 }} \mathbb{E}_{\text {(特征, 标签) }} \text { 数据分布 }[\text { 损失函数 (标签, 模型 (特征))] }$$
强化学习：


- 有监督学习和强化学习的优化目标相似，即都是在优化某个数据分布下的一个分数值的期望。
- 二者优化的途径是不同的，有监督学习直接通过优化模型对于数据特征的输出来优化目标，即修改目标函数而数据分布不变；**强化学习则通过改变策略来调整智能体和环境交互数据的分布**，进而优化目标，即修改数据分布而目标函数不变。

# 2. 多臂老虎机

### 2.2.3 累积懊悔

## 2.4 ε-贪心算法

每次以概率 $1-\epsilon$ 选择以往经验中期望奖励估值最大的那根拉杆（利用），以概率 $\epsilon$ 随机选择一根拉杆（探索）

随着探索次数的不断增加，我们对各个动作的奖励估计得越来越准，因此可以降低 $\epsilon$ 

## 2.5 上置信界算法

## 2.6 汤普森采样算法

