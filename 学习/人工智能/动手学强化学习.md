[初探强化学习 (boyuai.com)](https://hrl.boyuai.com/chapter/1/%E5%88%9D%E6%8E%A2%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/)

# 1. 初步了解
## 1.2 什么是强化学习

- 强化学习是机器通过**与环境交互**来实现目标的一种计算方法。
	- 机器和环境的一轮交互是指，机器在环境的一个状态下做一个动作决策，把这个动作作用到环境当中，这个环境发生相应的改变并且将**相应的奖励反馈和下一轮状态**传回机器。
	- 这种交互是迭代进行的，机器的目标是最大化在多轮交互过程中**获得的累积奖励的期望**。
- 强化学习用**智能体（agent）** 这个概念来表示做决策的机器。
	- 相比于有监督学习中的“模型”，强化学习中的“智能体”强调机器不但可以感知周围的环境信息，还可以通过做决策来直接改变这个环境，而不只是给出一些预测信号。
	- 智能体的三个关键因素：**感知、决策、奖励**
- 和监督学习的区别
	- 强化学习有多轮决策，即序贯决策；而预测任务通常是单轮任务
	- 智能体每轮做决策时考虑未来环境相应的改变，所以当前轮带来最大奖励反馈的动作，在长期来看并不一定是最优的。


## 1.3 强化学习的环境

$$
环境下一状态 \sim P(\cdot | 环境当前状态，智能体的动作) 
$$

与面向决策任务的智能体进行交互的环境是一个动态的随机过程，其未来状态的分布由当前状态和智能体决策的动作来共同决定，并且每一轮状态转移都伴随着两方面的随机性：一是智能体决策的动作的随机性，二是环境基于当前状态和智能体动作来采样下一刻状态的随机性。通过对环境的动态随机过程的刻画，我们能清楚地感受到，在动态随机过程中学习和在一个固定的数据分布下学习是非常不同的。


## 1.4 强化学习的目标

强化学习和有监督学习的学习目标其实是一致的，即在某个数据分布下优化一个分数值的期望。不过，经过后面的分析我们会发现，强化学习和有监督学习的优化途径是不同的。

智能体和环境每次进行交互时，环境会产生相应的奖励信号，其往往由实数标量来表示。这个奖励信号一般是诠释当前状态或动作的好坏的及时反馈信号。

整个交互过程的每一轮获得的奖励信号可以进行累加，形成智能体的整体回报（return）。


## 1.5 强化学习中的数据

监督学习中，任务建立在从给定的数据分布中采样得到的训练数据集上，通过优化在训练数据集中设定的目标函数（如最小化预测误差）来找到模型的最优参数。这里，训练数据集背后的数据分布是完全不变的。

强化学习中，**数据是在智能体与环境交互的过程中得到的**。如果智能体不采取某个决策动作，那么该动作对应的数据就永远无法被观测到，所以当前智能体的训练数据来自之前智能体的决策结果。因此，智能体的策略不同，与环境交互所产生的数据分布就不同。

**占用度量（occupancy measure）**: 归一化的占用度量用于衡量在一个智能体决策与一个动态环境的交互过程中，采样到一个具体的状态动作对（state-action pair）的概率分布。

- 强化学习的策略在训练中会不断更新，其对应的数据分布（即占用度量）也会相应地改变。
- 由于奖励建立在状态动作对之上，一个策略对应的价值其实就是一个占用度量下对应的奖励的期望，因此寻找最优策略对应着寻找最优占用度量。

## 1.6 强化学习与监督学习

监督学习：
$$\text { 最优模型 }=\arg \min _{\text {模型 }} \mathbb{E}_{\text {(特征, 标签) }} \text { 数据分布 }[\text { 损失函数 (标签, 模型 (特征))] }$$
强化学习：


- 有监督学习和强化学习的优化目标相似，即都是在优化某个数据分布下的一个分数值的期望。
- 二者优化的途径是不同的，有监督学习直接通过优化模型对于数据特征的输出来优化目标，即修改目标函数而数据分布不变；**强化学习则通过改变策略来调整智能体和环境交互数据的分布**，进而优化目标，即修改数据分布而目标函数不变。

# 2. 多臂老虎机

多臂老虎机问题与强化学习的一大区别在于其与环境的交互并不会改变环境，即多臂老虎机的每次交互的结果和以往的动作无关，所以可看作**无状态的强化学习**（stateless reinforcement learning）。

### 2.2.3 累积懊悔

## 2.4 ε-贪心算法

每次以概率 $1-\epsilon$ 选择以往经验中期望奖励估值最大的那根拉杆（利用），以概率 $\epsilon$ 随机选择一根拉杆（探索）

随着探索次数的不断增加，我们对各个动作的奖励估计得越来越准，因此可以降低 $\epsilon$ 

## 2.5 上置信界算法

## 2.6 汤普森采样算法


# 3. 马尔可夫决策过程

Markov decision process，MDP。强化学习中的环境一般就是一个马尔可夫决策过程。

马尔可夫决策过程包含状态信息以及状态之间的转移机制。如果要用强化学习去解决一个实际问题，第一步要做的事情就是把这个实际问题抽象为一个马尔可夫决策过程。

## 3.2 马尔可夫过程
### 3.2.1 随机过程

**随机过程**（stochastic process）是概率论的“动力学”部分。

所有可能的状态集合 $S$ 。随机现象在某时刻 $t$ 的取值，就是一个状态 $S_t$ 。

已知历史信息 $(S_1,\dots,S_t)$ 的下一个时刻状态为 $S_{t+1}$ 的概率为 $P(S_{t+1}|S_1,\dots,S_t)$ 

### 3.2.2 马尔可夫性质

$$
P(S_{t+1}|S_t) = P(S_{t+1}|S_1,\dots,S_t)
$$

当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有**马尔可夫性质**（Markov property）。

通过这种链式的关系，历史的信息被传递到了现在。马尔可夫性可以大大简化运算，因为**只要当前状态可知，所有的历史信息都不再需要了**。

### 3.2.3 马尔可夫过程

**马尔可夫过程**（Markov process）指具有马尔可夫性质的随机过程，也被称为**马尔可夫链**（Markov chain）。

用元组 $\left< \mathcal{S} ,\mathcal{P} \right>$ 描述，表示状态集合与状态转移矩阵表示。


## 3.3 马尔可夫奖励过程

在马尔可夫过程的基础上加入奖励函数 $r$ 和折扣因子 $\gamma$ ，就可以得到**马尔可夫奖励过程**（Markov reward process）。

$\left< \mathcal{S},\mathcal{P},\mathcal{r},\gamma \right>$ 

### 3.3.1 回报

在一个马尔可夫奖励过程中，从第 $t$ 时刻状态 $S_t$ 开始，直到终止状态时，所有奖励的衰减之和称为**回报** $G_t$（Return）。

### 3.3.2 价值函数

在马尔可夫奖励过程中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的**价值**（value）。所有状态的价值就组成了**价值函数**（value function）。

## 3.4 马尔可夫决策过程

如果有一个**外界的“刺激”**来共同改变这个随机过程，就有了**马尔可夫决策过程**（Markov decision process，MDP）。我们将这个来自外界的刺激称为**智能体**（agent）的动作，在马尔可夫奖励过程（MRP）的基础上加入动作，就得到了马尔可夫决策过程（MDP）

$\left< \mathcal{S},\mathcal{A},\mathcal{P},\mathcal{r},\gamma \right>$ 

- $S$ 是状态的集合；
- $\mathcal{A}$ 是动作的集合；
- $\gamma$ 是折扣因子；
- $r(s,a)$是奖励函数，此时奖励可以同时取决于状态 $a$ 和动作 $s$ ，在奖励函数只取决于状态时，则退化为 $r(s)$ ；
- $P(s\prime|s,a)$ 是状态转移函数，表示在状态 $s$ 执行动作 $a$ 之后到达状态 $s\prime$ 的概率。

### 3.4.1 策略

智能体的策略 $\pi$ ，表示在输入状态 $s$ 的情况下采取动作 $a$ 的概率。
$$
\pi(a|s) = P(A_t = a|S_t = s)
$$

### 3.4.2 状态价值函数

用 $V^\pi(s)$  表示在 MDP 中基于策略 $\pi$ 的价值状态函数，定义为从状态 $s$ 出发遵循策略 $\pi$ 能获得的期望回报：
$$
V^\pi(s)=\mathbb{E}_\pi[G_t|S_t=s]
$$

### 3.4.3 动作价值函数

不同于 MRP，在 MDP 中，由于动作的存在，我们额外定义一个**动作价值函数**（action-value function）。我们用 $Q^\pi(s,a)$ 表示在 MDP 遵循策略 $\pi$ 时，对当前状态 $s$ 执行动作 $a$ 得到的期望回报：
$$
Q^\pi(s,a)=\mathbb{E}_\pi[G_t|S_t=s,A_t=a]
$$

状态价值函数和动作价值函数之间有很强的关系。

### 3.4.4 贝尔曼期望方程



## 3.5 蒙特卡罗方法


