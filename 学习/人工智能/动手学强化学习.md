[初探强化学习 (boyuai.com)](https://hrl.boyuai.com/chapter/1/%E5%88%9D%E6%8E%A2%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/)

# 第一章 初步了解
## 1.2 什么是强化学习

- 强化学习是机器通过**与环境交互**来实现目标的一种计算方法。
	- 机器和环境的一轮交互是指，机器在环境的一个状态下做一个动作决策，把这个动作作用到环境当中，这个环境发生相应的改变并且将**相应的奖励反馈和下一轮状态**传回机器。
	- 这种交互是迭代进行的，机器的目标是最大化在多轮交互过程中**获得的累积奖励的期望**。
- 强化学习用**智能体（agent）** 这个概念来表示做决策的机器。
	- 相比于有监督学习中的“模型”，强化学习中的“智能体”强调机器不但可以感知周围的环境信息，还可以通过做决策来直接改变这个环境，而不只是给出一些预测信号。
	- 智能体的三个关键因素：**感知、决策、奖励**
- 和监督学习的区别
	- 强化学习有多轮决策，即序贯决策；而预测任务通常是单轮任务
	- 智能体每轮做决策时考虑未来环境相应的改变，所以当前轮带来最大奖励反馈的动作，在长期来看并不一定是最优的。


## 1.3 强化学习的环境

$$
环境下一状态 \sim P(\cdot | 环境当前状态，智能体的动作) 
$$

与面向决策任务的智能体进行交互的环境是一个动态的随机过程，其未来状态的分布由当前状态和智能体决策的动作来共同决定，并且每一轮状态转移都伴随着两方面的随机性：一是智能体决策的动作的随机性，二是环境基于当前状态和智能体动作来采样下一刻状态的随机性。通过对环境的动态随机过程的刻画，我们能清楚地感受到，在动态随机过程中学习和在一个固定的数据分布下学习是非常不同的。


## 1.4 强化学习的目标

强化学习和有监督学习的学习目标其实是一致的，即在某个数据分布下优化一个分数值的期望。不过，经过后面的分析我们会发现，强化学习和有监督学习的优化途径是不同的。

智能体和环境每次进行交互时，环境会产生相应的奖励信号，其往往由实数标量来表示。这个奖励信号一般是诠释当前状态或动作的好坏的及时反馈信号。

整个交互过程的每一轮获得的奖励信号可以进行累加，形成智能体的整体回报（return）。


