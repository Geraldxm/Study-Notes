# 概率论与数理统计
## 霍夫丁不等式 

该不等式给出了随机变量的和与其期望值偏差的概率上限。

## KL散度 | 相对熵

KL散度（Kullback-Leibler Divergence），又称相对熵，是一种衡量两个概率分布差异的度量。它量化了一个概率分布 \( Q \) 与参考概率分布 \( P \) 之间的“距离”。KL散度的原理基于信息论，具体计算公式为：

$$ D_{KL}(P \parallel Q) = \sum_{i} P(i) \log \left( \frac{P(i)}{Q(i)} \right) $$

或

$$ D_{KL}(P \parallel Q) = \int_{-\infty}^{\infty} p(x) \log \left( \frac{p(x)}{q(x)} \right) dx $$

其中，\( P \) 和 \( Q \) 分别是两个概率分布，\( p(x) \) 和 \( q(x) \) 是它们的概率密度函数。KL散度非负，且在 \( P \) 和 \( Q \) 完全相同时为0。KL散度不对称，即 $D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P)$ 。


