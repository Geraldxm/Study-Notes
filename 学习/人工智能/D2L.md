# 2. 预备知识

- 概率密度函数，概率分布函数（对密度函数积分）
# 3. 线性神经网络
## 3.1 线性回归
- 广播机制
- 损失函数（loss function），一般为非负数，数值越小损失越小，完美预测时为0
	- 平方误差函数
- 梯度下降（Gradient Descent）：计算损失函数关于模型参数的导数（梯度），在损失函数递减的方向上更新参数
	- 小批量随机梯度下降（minibatch stochastic gradient descent）：解决每次更新参数需要遍历整个数据集的问题
## 3.4 softmax 回归
- 表示标签时若并不与类别之间的自然顺序有关，则采用独热编码
- 全连接层的参数开销：对d个输入以及q个输出的全连接层，参数开销为O(dq)。
	- 可以通过超参数n减少到O(dq/n)
- 未规范化的预测不能作为输出，因为没有限制输出数字的总和为1，且可能有负值
	- 因此用softmax函数处理
# 4. 多层感知机
- 线性模型意味着单调假设，很有可能会出错
- 多层感知机（MLP）：在网络中加入隐藏层来克服线性模型的限制，把许多全连接层堆叠在一起。
	- 如果中间层仍然使用仿射函数，那么多层仿射函数的效果仍然是仿射函数，同单层没有区别
	- **激活函数！**：对每个仿射变换后使用非线性的激活函数，防止退化
- 模型选择、欠拟合和过拟合
	- 将模型在训练数据上拟合的比在潜在分布中更接近的现象称为*过拟合*（overfitting），用于对抗过拟合的技术称为*正则化*（regularization）。
- 模型复杂性
	- 更多参数的、参数更大取值范围的、更多迭代次数的模型更加复杂
- 导致模型（过拟合）的因素
	- 可调参数量（大）
	- 参数采用值（大）
	- 训练样本数量（少）
- 权重衰减
	- 我们需要一个更细粒度的工具来调整函数的复杂性，使其达到一个合适的平衡位置
	- 是一个广泛使用的正则化技术之一
	- $$L(\mathbf{w}, b) + \frac{\lambda}{2} \|\mathbf{w}\|^2,$$
	- 更新 $\mathbf{w}$ 的同时尝试将它缩小为0。较小的 λ 对应较小约束的 w，较大 λ 对应较大约束的 w
	- 通常网络输出层的偏置项 b 不会被正则化
- （Bias-Variance Tradeoff）偏差-方差权衡
	- 泛化性和灵活性之间的权衡
	- 线性模型有很高的偏差，但是方差很低
	- 深度神经网络学习特征间的交互，容易产生严重的过拟合
- 暂退法（Dropout）
	- 我们希望模型深度挖掘特征，**将权重分散到许多特征中**，而不是依赖少数潜在的虚假关联
	- 我们希望模型对输入的微小变化不应该敏感，也就是对其添加一些随机噪声应该无影响
	- 暂退法：在前向传播过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术。
		- 暂退法在前向传播过程中，计算每一内部层的同时丢弃一些神经元。
		* 暂退法可以避免过拟合，它通常与控制权重向量的维数和大小结合使用的。
		* 暂退法将活性值$h$替换为具有期望值$h$的随机变量。
		* 暂退法仅在训练期间使用
	* 为什么测试时不用dropout？
		* 因为在测试阶段，我们希望模型能够**利用其学习到的所有知识**来进行预测，而不是随机丢弃一部分知识。此外，由于测试阶段的数据通常不会像训练数据那样有噪声，因此过拟合在测试阶段不太可能发生。
		* 另外，还有一个原因是，dropout在训练阶段的随机性可以被视为对模型的一种**集成**（ensemble）。每次训练迭代，由于随机丢弃的神经元不同，模型**实际上都在学习不同的子网络**。在测试阶段，我们通过保留所有神经元，实际上是在**对所有这些子网络进行平均**，从而得到更稳定和可靠的预测结果。
- 前向传播（Forward Propagation）和反向传播（Backward Propagation）
	* 前向传播在神经网络定义的计算图中按顺序计算和存储中间变量，它的顺序是从输入层到输出层。
	* 反向传播按相反的顺序（从输出层到输入层）计算和存储神经网络的中间变量和参数的梯度。
	* 在训练深度学习模型时，前向传播和反向传播是相互依赖的。
	* 训练比预测需要更多的内存。
* 梯度爆炸和梯度消失
	* 太多概率乘在一起，容易产生数值下溢上溢问题
	* 梯度爆炸，参数更新过大，破坏模型的收敛
	* 梯度消失，参数更新过小，几乎不会移动，无法学习
	* 梯度消失和梯度爆炸是深度网络中常见的问题。在参数初始化时需要非常小心，以确保梯度和参数可以得到很好的控制。
	* 需要用启发式的初始化方法来确保初始梯度既不太大也不太小。
	* ReLU激活函数缓解了梯度消失问题，这样可以加速收敛。
	* 随机初始化是保证在进行优化前打破对称性的关键。
	* Xavier初始化表明，对于每一层，输出的方差不受输入数量的影响，任何梯度的方差不受输出数量的影响。
* 环境和分布偏移
	* 协变量偏移：输入的分布改变，标签函数没有改变。例如，我们可能使用在夏天收集的数据来训练一个预测天气的模型，然后在冬天使用这个模型。在这种情况下，输入数据（例如温度和湿度）的分布会发生变化，但给定这些输入时，天气（晴、雨等）的概率分布可能保持不变。
	
	* 标签偏移：标签偏移是指训练数据和测试数据中标签（或输出）的分布不同，但在给定标签时，输入（或特征）的条件概率分布保持不变。这是一种特殊类型的分布偏移，它假设输入和标签之间的关系在训练和测试数据中保持不变，只是标签本身的分布发生了变化。

		例如，假设你正在训练一个垃圾邮件分类器。在你的训练数据中，90%的邮件是垃圾邮件，10%的邮件是正常邮件。然而，在实际应用中，你可能只有1%的邮件是垃圾邮件，99%的邮件是正常邮件。这就是一个标签偏移的例子，因为标签（即邮件是否是垃圾邮件）的分布在训练数据和测试数据中是不同的。
		
		在处理标签偏移时，一个常见的策略是重新调整训练样本的权重，使得训练数据中的标签分布更接近测试数据中的标签分布。这种方法通常被称为重标定或重采样。然而，这种方法的前提是我们有足够的知识或数据来估计测试数据中的标签分布，这在实际应用中可能并不总是可行的。
		
	* 概念偏移：概念偏移是指训练数据和测试数据的条件概率（即给定输入时标签的概率，或给定标签时输入的概率）不同
	* 在许多情况下，**训练集和测试集并不来自同一个分布**。这就是所谓的分布偏移。
	* 真实风险是从真实分布中抽取的所有数据的总体损失的预期。然而，这个数据总体通常是无法获得的。经验风险是训练数据的平均损失，用于近似真实风险。在实践中，我们进行经验风险最小化。
	* 在相应的假设条件下，可以在测试时检测并纠正协变量偏移和标签偏移。在测试时，不考虑这种偏移可能会成为问题。
* 真实场景预测——数据预处理
	* 将所有缺失的值替换为相应特征的平均值
	* 通过将特征重新缩放到零均值和单位方差来标准化数据
	* 处理离散值，用独热编码替换
# 5. 深度学习计算
- 一个块可以由许多层组成；一个块可以由许多块组成。
- 块可以包含代码。
- 块负责大量的内部处理，包括参数初始化和反向传播。
- 层和块的顺序连接由`Sequential`块处理。

## 5.2 参数管理
- 访问参数，调试、诊断和可视化
- 参数初始化
- 不同模型组件间的参数共享
	- 有时我们希望在多个层间共享参数： 我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。
## 5.6 GPU
* 我们可以指定用于存储和计算的设备，例如CPU或GPU。默认情况下，数据在主内存中创建，然后使用CPU进行计算。
* 深度学习框架要求计算的所有输入数据都在同一设备上，无论是CPU还是GPU。
* 不经意地移动数据可能会显著降低性能。一个典型的错误如下：计算GPU上每个小批量的损失，并在命令行中将其报告给用户（或将其记录在NumPy `ndarray`中）时，将触发全局解释器锁，从而使所有GPU阻塞。最好是为GPU内部的日志分配内存，并且只移动较大的日志。
# 6. 卷积神经网路
- 以前，多层感知机可能需要数十亿个参数来表示网络中的一层，而现在卷积神经网络通常只需要几百个参数，而且不需要改变输入或隐藏表示的维数。 参数大幅减少的代价是，我们的特征现在是平移不变的，并且当确定每个隐藏活性值时，每一层只包含局部的信息。 以上所有的权重学习都将依赖于归纳偏置。当这种偏置与现实相符时，我们就能得到样本有效的模型，并且这些模型能很好地泛化到未知数据中。 但如果这偏置与现实不符时，比如当图像不满足平移不变时，我们的模型可能难以拟合我们的训练数据。

- 图像的平移不变性使我们以相同的方式处理局部图像，而不在乎它的位置。
- 局部性意味着计算相应的隐藏表示只需一小部分局部图像像素。
- 在图像处理中，卷积层通常比全连接层需要更少的参数，但依旧获得高效用的模型。
- 卷积神经网络（CNN）是一类特殊的神经网络，它可以包含多个卷积层。
- 多个输入和输出通道使模型在每个空间位置可以获取图像的多方面特征。

* 填充可以增加输出的高度和宽度。这常用来使输出与输入具有相同的高和宽。
* 步幅可以减小输出的高和宽，例如输出的高和宽仅为输入的高和宽的$1/n$（$n$是一个大于$1$的整数）。
* 填充和步幅可用于有效地调整数据的维度。
## 6.4 多输入输出通道
- 在最流行的神经网络架构中，随着神经网络层数的加深，我们常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。直观地说，**我们可以将每个通道看作对不同特征的响应。** 
- 
* 当以每像素为基础应用时，$1\times 1$卷积层相当于全连接层。
* $1\times 1$卷积层通常用于调整网络层的通道数量和控制模型复杂性。
## 6.5 Pooling 池化
 - 具有双重目的：降低卷积层对位置的敏感性（如摄像机震动导致的像素移动），同时降低对空间降采样表示的敏感性。
* 对于给定输入元素，最大汇聚层会输出该窗口内的最大值，平均汇聚层会输出该窗口内的平均值。
* 汇聚层的主要优点之一是减轻卷积层对位置的过度敏感。
* 我们可以指定汇聚层的填充和步幅。
* 使用最大汇聚层以及大于1的步幅，可减少空间维度（如高度和宽度）
* 汇聚层的输出通道数与输入通道数相同。

## 6.6 LeNet
* 卷积神经网络（CNN）是一类使用卷积层的网络。
* 在卷积神经网络中，我们组合使用卷积层、非线性激活函数和汇聚层。
* 为了构造高性能的卷积神经网络，我们通常对卷积层进行排列，逐渐降低其表示的空间分辨率，同时增加通道数。
* 在传统的卷积神经网络中，卷积块编码得到的表征在输出之前需由一个或多个全连接层进行处理。
* LeNet是最早发布的卷积神经网络之一。
# 7. 现代卷积神经网络
## 7.1 AlexNet
- 深度卷积神经网络的突破出现在2012年。突破可归因于两个关键因素
	- 数据：2009年，ImageNet数据集发布，并发起ImageNet挑战赛：要求研究人员从100万个样本中训练模型，以区分1000个不同类别的对象。ImageNet数据集由斯坦福教授李飞飞小组的研究人员开发，利用谷歌图像搜索（Google Image Search）对每一类图像进行预筛选，并利用亚马逊众包（Amazon Mechanical Turk）来标注每张图片的相关类别。这种规模是前所未有的。这项被称为ImageNet的挑战赛推动了计算机视觉和机器学习研究的发展，挑战研究人员确定哪些模型能够在更大的数据规模下表现最好。
	- 硬件：GPU
- AlexNet
	- AlexNet将sigmoid激活函数改为更简单的ReLU激活函数。
	- AlexNet通过暂退法控制全连接层的模型复杂度，而LeNet只使用了权重衰减。
	- AlexNet在训练时增加了大量的图像增强数据，如翻转、裁切和变色。
	* AlexNet的架构与LeNet相似，但使用了更多的卷积层和更多的参数来拟合大规模的ImageNet数据集。
* 今天，AlexNet已经被更有效的架构所超越，但它是从浅层网络到深层网络的关键一步。
* 尽管AlexNet的代码只比LeNet多出几行，但学术界花了很多年才接受深度学习这一概念，并应用其出色的实验结果。这也是由于缺乏有效的计算工具。
* Dropout、ReLU和预处理是提升计算机视觉任务性能的其他关键步骤。
## 7.2 VGG 使用块的网络
1. 带填充以保持分辨率的卷积层；
2. 非线性激活函数，如ReLU；
3. 汇聚层，如最大汇聚层。
* VGG-11使用可复用的卷积块构造网络。不同的VGG模型可通过每个块中卷积层数量和输出通道数量的差异来定义。

* 块的使用导致网络定义的非常简洁。使用块可以有效地设计复杂的网络。

* 在VGG论文中，Simonyan和Ziserman尝试了各种架构。特别是他们发现深层且窄的卷积（即$3 \times 3$）比较浅层且宽的卷积更有效。
## 7.3 NiN 网络中的网络
* NiN使用由一个卷积层和多个$1\times 1$卷积层组成的块。该块可以在卷积神经网络中使用，以允许更多的每像素非线性。

* NiN去除了容易造成过拟合的全连接层，将它们替换为全局平均汇聚层（即在所有位置上进行求和）。该汇聚层通道数量为所需的输出数量（例如，Fashion-MNIST的输出为10）。

* 移除全连接层可减少过拟合，同时显著减少NiN的参数。

* NiN的设计影响了许多后续卷积神经网络的设计。
## 7.4 googleNet
## 7.5 batch-norm
- 一种流行且有效的技术，可持续加速深层网络的收敛速度。
- 批量规范化使得研究人员能够训练100层以上的网络。
## 7.6 ResNet 残差网络
- 对于深度神经网络，如果我们能将新添加的层训练成*恒等映射*（identity function）$f(\mathbf{x}) = \mathbf{x}$，新模型和原模型将同样有效。
- 残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一。
## 7.7 DenseNet  稠密连接网络
- 是ResNet的逻辑扩展
- ResNet和DenseNet的关键区别在于，DenseNet输出是*连接*，而不是如ResNet的简单相加。
$$\mathbf{x} \to \left[

\mathbf{x},

f_1(\mathbf{x}),

f_2([\mathbf{x}, f_1(\mathbf{x})]), f_3([\mathbf{x}, f_1(\mathbf{x}), f_2([\mathbf{x}, f_1(\mathbf{x})])]), \ldots\right].$$
- DenseNet这个名字由变量之间的“稠密连接”而得来，最后一层与之前的所有层紧密相连。
- 稠密网络主要由2部分构成：*稠密块*（dense block）和*过渡层*（transition layer）。
- 前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。

# 13. 计算机视觉
## 8.1 image-augmentation 图像增广
- 翻转（上下、左右）、裁切（大小、宽高比）
- 颜色（亮度、对比度、饱和度、色调）
## 8.2 fine-tuning 微调
微调包括以下四个步骤。
1. 在源数据集（例如ImageNet数据集）上预训练神经网络模型，即*源模型*。
2. 创建一个新的神经网络模型，即*目标模型*。这将复制源模型上的所有模型设计及其参数（输出层除外）。我们假定这些模型参数包含从源数据集中学到的知识，这些知识也将适用于目标数据集。我们还假设源模型的输出层与源数据集的标签密切相关；因此不在目标模型中使用该层。
3. 向目标模型添加输出层，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数。
4. 在目标数据集（如椅子数据集）上训练目标模型。输出层将从头开始进行训练，而所有其他层的参数将根据源模型的参数进行微调。
## 8.3 bounding-box 边界框
## 8.4 anchor 锚框
- **在训练集中，我们将每个锚框视为一个训练样本。为了训练目标检测模型，我们需要每个锚框的*类别*（class）和*偏移量*（offset）标签，其中前者是与锚框相关的对象的类别，后者是真实边界框相对于锚框的偏移量。在预测时，我们为每个图像生成多个锚框，预测所有锚框的类别和偏移量，根据预测的偏移量调整它们的位置以获得预测的边界框，最后只输出符合特定条件的预测边界框。** 
 
- 要生成多个不同形状的锚框，让我们设置许多缩放比（scale）取值$s_1,\ldots, s_n$和许多宽高比（aspect ratio）取值$r_1,\ldots, r_m$。
- 在实践中，(**我们只考虑**)包含$s_1$或$r_1$的(**组合：**)
- IoU：$$J(\mathcal{A},\mathcal{B}) = \frac{\left|\mathcal{A} \cap \mathcal{B}\right|}{\left| \mathcal{A} \cup \mathcal{B}\right|}.$$
- 要标记任何生成的锚框，我们可以参考分配到的最接近此锚框的真实边界框的位置和类别标签。
- NMS：非极大值抑制
## 8.5 多尺度目标检测

## 8.7 SSD 单发多框检测
- 类别预测层
	- 在某个尺度下，设特征图的高和宽分别为$h$和$w$。如果以其中每个单元为中心生成$a$个锚框，那么我们需要对$hwa$个锚框进行分类。 
	- 虑输出和输入同一空间坐标（$x$、$y$）：输出特征图上（$x$、$y$）坐标的通道里包含了以输入特征图（$x$、$y$）坐标为中心生成的所有锚框的类别预测。
	- 因此输出通道数为$a(q+1)$，其中索引为$i(q+1) + j$（$0 \leq j \leq q$）的通道代表了索引为$i$的锚框有关类别索引为$j$的预测。
- 边界框预测层
	- 边界框预测层的设计与类别预测层的设计类似。
	- 唯一不同的是，这里需要为每个锚框预测4个偏移量，而不是$q+1$个类别。
## 8.8 R-CNN 区域卷积神经网络
- Fast R-CNN
- Faster R-CNN
- Mask R-CNN
* R-CNN对图像选取若干提议区域，使用卷积神经网络对每个提议区域执行前向传播以抽取其特征，然后再用这些特征来预测提议区域的类别和边界框。
* Fast R-CNN对R-CNN的一个主要改进：只对整个图像做卷积神经网络的前向传播。它还引入了兴趣区域汇聚层，从而为具有不同形状的兴趣区域抽取相同形状的特征。
* Faster R-CNN将Fast R-CNN中使用的选择性搜索替换为参与训练的区域提议网络，这样后者可以在减少提议区域数量的情况下仍保证目标检测的精度。
* Mask R-CNN在Faster R-CNN的基础上引入了一个全卷积网络，从而借助目标的像素级位置进一步提升目标检测的精度。
## 8.9 语义分割
- 与目标检测不同，语义分割可以识别并理解图像中每一个像素的内容：其语义区域的标注和预测是像素级的。
- 区别于图像分割、实例分割
	* *图像分割*将图像划分为若干组成区域，这类问题的方法通常利用图像中像素之间的相关性。它在训练时**不需要有关图像像素的标签信息**，在预测时也无法保证分割出的区域具有我们希望得到的语义。
		* 图像分割可能会将狗分为两个区域：一个覆盖以黑色为主的嘴和眼睛，另一个覆盖以黄色为主的其余部分身体。
	* *实例分割*也叫*同时检测并分割*（simultaneous detection and segmentation），它研究如何识别图像中各个目标实例的像素级区域。与语义分割不同，实例分割**不仅需要区分语义，还要区分不同的目标实例**。
		* 如果图像中有两条狗，则实例分割需要区分像素属于的两条狗中的哪一条。
## 8.10 转置卷积
- 用于逆转下采样导致的空间尺寸减小
## 8.11 FCN 全卷积网络
- 全卷积网络将中间层特征图的高和宽变换回输入图像的尺寸：这是通过*转置卷积*（transposed convolution）实现的。
* 全卷积网络先使用卷积神经网络抽取图像特征，然后通过$1\times 1$卷积层将通道数变换为类别个数，最后通过转置卷积层将特征图的高和宽变换为输入图像的尺寸。
* 在全卷积网络中，我们可以将转置卷积层初始化为双线性插值的上采样。
## 8.12 neural-style 风格迁移

# 8. 循环神经网络
## 8.1 序列模型
- 内插法（在现有观测值之间进行估计）和外推法（对超出已知观测范围进行预测）在实践的难度上差别很大。因此，对于所拥有的序列数据，在训练时始终要尊重其时间顺序，即最好不要基于未来的数据进行训练。
- 对于时间是向前推进的因果模型，正向估计通常比反向估计更容易。
## 8.2 文本预处理
- 我们构建一个字典，通常也叫做_词表_（vocabulary）， 用来将字符串类型的词元映射到从0开始的数字索引中
## 8.3 语言模型与数据集
- 语言模型是自然语言处理的关键。
- �元语法通过截断相关性，为处理长序列提供了一种实用的模型。
- 长序列存在一个问题：它们很少出现或者从不出现。
- 齐普夫定律支配着单词的分布，这个分布不仅适用于一元语法，还适用于其他�元语法。
- 通过拉普拉斯平滑法可以有效地处理结构丰富而频率不足的低频词词组。
- 读取长序列的主要方式是随机采样和顺序分区。在迭代过程中，后者可以保证来自两个相邻的小批量中的子序列在原始序列上也是相邻的。

# 0. 笔记
## 0.1 损失函数 Loss
- 损失函数在神经网络中具有重要作用，它主要用于衡量模型预测结果与真实标签之间的差异或错误程度。它是用来定义和量化训练目标的一种数学表达方式。
- ![[Pasted image 20240427134657.png]]
- ![[Pasted image 20240427134943.png]]
### 0.1.1 MSELoss 平均平方误差损失
- MSELoss（平均平方误差损失）
- ![[Pasted image 20240427140509.png]]
### 0.1.2 CELoss 交叉熵损失
- Cross Entropy Loss，特别适用于分类问题
- 对于一个分类问题，假设有 𝐶 个类别 (𝐶≥2) ，模型的输出为一个概率分布向量（通常使用 Softmax 函数将原始输出转化为概率），表示每个类别的预测概率。真实标签则表示为一个独热编码向量或者类别索引。
- ![[Pasted image 20240427140128.png]]
- 
### 0.1.3 BCELoss 二元交叉熵损失
- ![[Pasted image 20240427141552.png]]
### 0.1.4 LnLoss
1. **L1Loss**：也被称为绝对值损失或者MAE（Mean Absolute Error），它计算的是预测值和真实值之间的绝对差的平均值。具体来说，如果我们的预测值是$\hat{y}$，真实值是$y$，那么L1损失可以定义为：

   $$L1Loss = \frac{1}{n}\sum_{i=1}^{n}|\hat{y}_i - y_i|$$

   其中$n$是样本数量。L1损失对于异常值不敏感，因此在处理带有噪声的数据时，L1损失通常会有较好的效果。

2. **L2Loss**：也被称为平方损失或者MSE（Mean Squared Error），它计算的是预测值和真实值之间的平方差的平均值。具体来说，L2损失可以定义为：

   $$L2Loss = \frac{1}{n}\sum_{i=1}^{n}(\hat{y}_i - y_i)^2$$

   L2损失对于异常值非常敏感，因为它会对较大的误差进行更大的惩罚。因此，如果你的数据中没有噪声或者异常值，那么使用L2损失可能会得到更好的效果。
## 0.2 梯度下降 Gradient Descent
梯度下降是一种常用的优化算法，用于更新模型参数以最小化损失函数。具体操作如下：
1. **计算损失函数的梯度**：首先，计算当前参数下损失函数的梯度。这个梯度指示了在参数空间中损失函数增长最快的方向。
   
2. **更新参数**：为了减少损失，需要在参数空间中向梯度的反方向移动。因此，参数更新公式为：
   $$
   \theta = \theta - \eta \nabla J(\theta)
   $$
   其中，$\theta$ 表示模型参数，$\eta$ 是学习率（一个小的正数），$\nabla J(\theta)$ 是损失函数$J$关于参数$\theta$的梯度。
### 0.2.1 批量梯度下降 Batch Gradient Descnet
- 最传统的形式，在每一次迭代中使用整个训练集来计算损失函数的梯度
**优点**：
- 梯度计算准确，每次迭代都沿着确切的最陡下降方向更新参数，收敛到全局最小值（对凸问题）或局部最小值。
**缺点**：
- 计算成本高，特别是当训练集非常大时。
- 更新速度慢，每次更新需要遍历整个数据集。

### 0.2.2 随机梯度下降 Stochastic Gradient Descent
- 每一次迭代中，随机选择一个训练样本来计算梯度（每次训练都会使用所有单个样本更新模型参数）
**优点**：
- 计算速度快，每次迭代只处理一个样本。
- 有助于跳出局部最小值，增加算法寻找全局最小值的可能性。
**缺点**：
- 更新过程中引入噪声，收敛过程不稳定，梯度方向有很大波动。
- 可能永远都不会真正收敛
- 单样本难以利用所有的硬件资源（造成浪费），GPU有上千核，因此一般使用的是小批量随机梯度下降

### 0.2.3 小批量随机梯度下降 Mini-Batch SGD
小批量梯度下降结合了批量梯度下降和随机梯度下降的优点。它在每次迭代中使用一个小批量（batch）的样本来计算梯度：
$$
\theta = \theta - \eta \nabla J(\theta; \text{batch})
$$
**优点**：
- 通过适当选择批量大小，可以在计算效率和梯度估计精度之间达到平衡。
- 有助于利用高效的矩阵运算优化。

## 0.3 经典公式
### 0.3.1 Softmax函数

- [Softmax 函数详解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/685421257)
- ![[Pasted image 20240427135843.png]]
- 