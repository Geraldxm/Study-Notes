# 第一章 绪论
- 数据决定模型的效果的上限，算法则是让模型无限逼近上限
- 假设空间和版本空间
# 第二章 模型评估与选择
## 2.2 评估方法
- 留出法
- 分层采样：保留类别比例
- 交叉验证法：k折交叉验证，10次10折交叉验证法
- 留一法
- bootstrap 自助法：
	- 自助采样(bootstrap sampling)
## 2.3 性能度量
### 回归任务
- 均方误差(Mean Squared Error, MSE)
### 分类任务
- 查准率(precision)和查全率(recall)
	- 查准率和查全率是一对矛盾的度量
	- 可以画出P-R曲线，如果一个PR曲线完全包住了另一个曲线，那么前者性能更优；若有交叉，则不能直接断言，因此通常用“平衡点 ”(Break-Even Point, BEP)，也就是P=R的这条射线，和曲线的交点，来判断不同模型的性能
	- 也可以用F1度量
- 宏P、R、F1
- 微P、R、F1
- ROC(Receiver Operating Characteristic，受试者工作特征)
	- 横轴为假正例率，纵轴为真正例率
- AUC(Area Under ROC Curve)
	- 即ROC曲线下各部分面积求和而得
- 代价敏感错误率与错误曲线

## 2.4 比较检验
## 2.5 偏差与方差
- **“偏差-方差分解”** 是解释学习算法泛化性能的一种重要工具。
- 根据推导，泛化误差 = 偏差 + 方差 + 噪声
- **偏差**度量了学习算法的期望预测与真实结果的偏离程度，也就是学习算法本身的拟合能力
- **方差**度量了同样大小的训练集变动导致的学习性能变化，刻画了数据扰动造成的影响
- **噪声**表达了当前任务上任何学习算法所能达到的期望泛化误差的下界，刻画了学习问题本身的难度
- 一般来说，偏差与方差有冲突，称为**偏差-方差窘境(bias-variance dilemma)** 
	- 训练不足时，学习器拟合能力弱，偏差占主导
	- 训练充足时，训练数据的轻微扰动可能导致学习器变化很大，容易发生过拟合，方差占主导

# 第三章 线性模型
## 3.2 线性回归

- 线性回归：实际上就是求解w和b使 $E_{(w, b)}=\sum_{i=1}^m\left(y_i-w x_i-b\right)^2$ 最小化的过程
- 均方误差：对应了“欧氏距离”
- 对数线性回归：$\ln y=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b$ 
- 广义线性模型：$y=g^{-1}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right)$ 
### 最小二乘法
- 基于均方误差最小化，来进行模型求解的方法称为“最小二乘法”
- 在线性回归中，最小二乘法就是找到一条直线，使所有样本到直线上的欧式距离之和最小
### 多元线性回归

若使：
$$
\mathbf{X}=\left(\begin{array}{ccccc}
x_{11} & x_{12} & \ldots & x_{1 d} & 1 \\
x_{21} & x_{22} & \ldots & x_{2 d} & 1 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
x_{m 1} & x_{m 2} & \ldots & x_{m d} & 1
\end{array}\right)=\left(\begin{array}{cc}
\boldsymbol{x}_1^{\mathrm{T}} & 1 \\
\boldsymbol{x}_2^{\mathrm{T}} & 1 \\
\vdots & \vdots \\
\boldsymbol{x}_m^{\mathrm{T}} & 1
\end{array}\right)
$$\
$$
\hat{\boldsymbol{w}}=(\boldsymbol{w} ; b)
$$
则有：
$$
\hat{\boldsymbol{w}}^*=\underset{\hat{\boldsymbol{w}}}{\arg \min }(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}}) .
$$
## 3.3 对数几率回归
$$
y=\frac{1}{1+e^{-\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right)}} .
$$
## 3.4 线性判别分析
## 3.5 多分类学习
- OvO, One vs. one
- OvR, One vs. Rest
- MvM, Many vs. Many
## 3.6 类别不平衡问题
- 无偏采样：意味着真实样本的类别比例在训练集中得以保持
- 再缩放/再平衡（基于“训练集是真实样本的无偏估计”）
- 主流三类做法：欠采样、过采样、阈值移动
- 过采样方法：
	- 不能简单地对初始正例样本进行重复采样
	- SMOTE：插值
- 欠采样方法：
	- 若随机丢弃，可能丢失重要信息
	- EasyhEnsemble：集成学习，反例划分不同集合
# 第四章 决策树
- 递归返回情形：
	1. 当前结点样本全属于同一类别，无需划分
	2. 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分
	3. 当前结点样本集合为空，不能划分
- 决策树的关键是如何选择最优划分属性
## 4.2 划分选择
### 信息增益划分 - ID3

- 信息熵：$\operatorname{Ent}(D)=-\sum_{k=1}^{|\mathcal{Y}|} p_k \log _2 p_k$ 
	- 取值范围0~log2|y|。y为类别数
	- 值越小，说明样本集合D的纯度越高
- **信息增益（互信息）**：
$$\operatorname{Gain}(D, a)=\operatorname{Ent}(D)-\sum_{v=1}^V \frac{\left|D^v\right|}{|D|} \operatorname{Ent}\left(D^v\right)\tag{4.2}$$ 
	- 已知属性a的取值后，样本类别这个随机变量的不确定性减小的程度
	- 信息增益越大，说明样本不确定性减小程度越大

### 增益率划分 - C4.5

- **信息增益会对可取值数目较多的属性有所偏好**，为了减少影响，C4.5决策树算法使用“增益率”（gain ratio）来选择最优划分属性
- 增益率：
$$\operatorname{Gain\_ ratio}(D,a)=\frac{\operatorname{Gain}(D, a)}{\operatorname{IV}(a)}$$
$$\operatorname{IV}(a)=-\sum_{v=1}^{V}\frac{|D^v|}{D}log_2\frac{|D^v|}{D}$$
	- IV为属性a的固有之，属性a下取值数目越多（V越大），则IV值通常越大
	- 增益率对取值数较少的属性有所偏好
	- 因此往往先从候选划分属性中，找到信息增益高于平均的属性，再从中选取增益率最高的

### 基尼指数 - CART

- CART(Classfication and Regression Tree)，分类和回归任务都可以使用。用“基尼系数”(Gini index)
- 数据集D的纯度可以用基尼值来度量
$$
\begin{aligned}
\operatorname{Gini}(D) & =\sum_{k=1}^{|\mathcal{Y}|} \sum_{k^{\prime} \neq k} p_k p_{k^{\prime}} \\
& =1-\sum_{k=1}^{|\mathcal{Y}|} p_k^2 .
\end{aligned}
$$
	- 直观来说，反映了从数据集D中随机抽取两个样本，类别标记不一样的概率。值越小，纯度越高。
- 属性a的基尼指数$$
\operatorname{Gini} \operatorname{index}(D, a)=\sum_{v=1}^V \frac{\left|D^v\right|}{|D|} \operatorname{Gini}\left(D^v\right)
$$

## 4.3 剪枝
- 防止决策树**过拟合**的主要手段。训练过程中，为了尽可能正确分类训练样本呢，可能导致分支过多，造成拟合
- 基本策略：预剪枝和后剪枝

## 4.4 连续与缺失值
### 连续值处理
- 可以采用二分方法，采用区间的中位点作为候选划分点，然后可以公式