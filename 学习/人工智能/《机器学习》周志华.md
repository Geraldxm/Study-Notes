# 第一章 绪论
- 数据决定模型的效果的上限，算法则是让模型无限逼近上限
- 假设空间和版本空间

# 第二章 模型评估与选择
## 2.2 评估方法
- 留出法
- 分层采样：保留类别比例
- 交叉验证法：k折交叉验证，10次10折交叉验证法
- 留一法
- bootstrap 自助法：
	- 自助采样(bootstrap sampling)
## 2.3 性能度量
### 回归任务
- 均方误差(Mean Squared Error, MSE)
### 分类任务
- 查准率(precision)和查全率(recall)
	- 查准率和查全率是一对矛盾的度量
	- 可以画出P-R曲线，如果一个PR曲线完全包住了另一个曲线，那么前者性能更优；若有交叉，则不能直接断言，因此通常用“平衡点 ”(Break-Even Point, BEP)，也就是P=R的这条射线，和曲线的交点，来判断不同模型的性能
	- 也可以用F1度量
- 宏P、R、F1
- 微P、R、F1
- ROC(Receiver Operating Characteristic，受试者工作特征)
	- 横轴为假正例率，纵轴为真正例率
- AUC(Area Under ROC Curve)
	- 即ROC曲线下各部分面积求和而得
- 代价敏感错误率与错误曲线

## 2.4 比较检验
## 2.5 偏差与方差
- **“偏差-方差分解”** 是解释学习算法泛化性能的一种重要工具。zui
- 根据推导，泛化误差 = 偏差 + 方差 + 噪声
- **偏差**度量了学习算法的期望预测与真实结果的偏离程度，也就是学习算法本身的拟合能力
- **方差**度量了同样大小的训练集变动导致的学习性能变化，刻画了数据扰动造成的影响
- **噪声**表达了当前任务上任何学习算法所能达到的期望泛化误差的下界，刻画了学习问题本身的难度
- 一般来说，偏差与方差有冲突，称为**偏差-方差窘境(bias-variance dilemma)** 
	- 训练不足时，学习器拟合能力弱，偏差占主导
	- 训练充足时，训练数据的轻微扰动可能导致学习器变化很大，容易发生过拟合，方差占主导

# 第三章 线性模型
## 3.2 线性回归

- 线性回归：实际上就是求解w和b使 $E_{(w, b)}=\sum_{i=1}^m\left(y_i-w x_i-b\right)^2$ 最小化的过程
- 均方误差：对应了“欧氏距离”
- 对数线性回归：$\ln y=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b$ 
- 广义线性模型：$y=g^{-1}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right)$ 
### 最小二乘法
- 基于均方误差最小化，来进行模型求解的方法称为“最小二乘法”
- 在线性回归中，最小二乘法就是找到一条直线，使所有样本到直线上的欧式距离之和最小
### 多元线性回归

若使：
$$
\mathbf{X}=\left(\begin{array}{ccccc}
x_{11} & x_{12} & \ldots & x_{1 d} & 1 \\
x_{21} & x_{22} & \ldots & x_{2 d} & 1 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
x_{m 1} & x_{m 2} & \ldots & x_{m d} & 1
\end{array}\right)=\left(\begin{array}{cc}
\boldsymbol{x}_1^{\mathrm{T}} & 1 \\
\boldsymbol{x}_2^{\mathrm{T}} & 1 \\
\vdots & \vdots \\
\boldsymbol{x}_m^{\mathrm{T}} & 1
\end{array}\right)
$$

$$
\hat{\boldsymbol{w}}=(\boldsymbol{w} ; b)
$$
则有：
$$
\hat{\boldsymbol{w}}^*=\underset{\hat{\boldsymbol{w}}}{\arg \min }(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}}) .
$$
## 3.3 对数几率回归
$$
y=\frac{1}{1+e^{-\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right)}} .
$$
## 3.4 线性判别分析
## 3.5 多分类学习
- OvO, One vs. one
- OvR, One vs. Rest
- MvM, Many vs. Many
## 3.6 类别不平衡问题
- 无偏采样：意味着真实样本的类别比例在训练集中得以保持
- 再缩放/再平衡（基于“训练集是真实样本的无偏估计”）
- 主流三类做法：欠采样、过采样、阈值移动
- 过采样方法：
	- 不能简单地对初始正例样本进行重复采样
	- SMOTE：插值
- 欠采样方法：
	- 若随机丢弃，可能丢失重要信息
	- EasyhEnsemble：集成学习，反例划分不同集合

# 第四章 决策树
- 递归返回情形：
	1. 当前结点样本全属于同一类别，无需划分
	2. 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分
	3. 当前结点样本集合为空，不能划分
- 决策树的关键是如何**选择最优划分属性**
## 4.2 划分选择
### 信息增益划分 - ID3

- 信息熵：$\operatorname{Ent}(D)=-\sum_{k=1}^{|\mathcal{Y}|} p_k \log _2 p_k$ 
	- 取值范围0~log2|y|。y为类别数
	- 值越小，说明样本集合D的纯度越高
- **信息增益（互信息）**：
$$\operatorname{Gain}(D, a)=\operatorname{Ent}(D)-\sum_{v=1}^V \frac{\left|D^v\right|}{|D|} \operatorname{Ent}\left(D^v\right)\tag{4.2}$$ 
	- 已知属性a的取值后，样本类别这个随机变量的不确定性减小的程度
	- 信息增益越大，说明样本不确定性减小程度越大

### 增益率划分 - C4.5

- **信息增益会对可取值数目较多的属性有所偏好**，为了减少影响，C4.5决策树算法使用“增益率”（gain ratio）来选择最优划分属性
- 增益率：
$$\operatorname{Gain\_ ratio}(D,a)=\frac{\operatorname{Gain}(D, a)}{\operatorname{IV}(a)}\tag{4.3}$$
$$\operatorname{IV}(a)=-\sum_{v=1}^{V}\frac{|D^v|}{D}log_2\frac{|D^v|}{D}\tag{4.4}$$
	- IV为属性a的固有之，属性a下取值数目越多（V越大），则IV值通常越大
	- 增益率对取值数较少的属性有所偏好
	- 因此往往先从候选划分属性中，找到信息增益高于平均的属性，再从中选取增益率最高的

### 基尼指数 - CART

- CART(Classfication and Regression Tree)，分类和回归任务都可以使用。用“基尼系数”(Gini index)
- 数据集D的纯度可以用基尼值来度量
$$
\begin{aligned}
\operatorname{Gini}(D) & =\sum_{k=1}^{|\mathcal{Y}|} \sum_{k^{\prime} \neq k} p_k p_{k^{\prime}} \\
& =1-\sum_{k=1}^{|\mathcal{Y}|} p_k^2 .
\end{aligned}
$$
	- 直观来说，反映了从数据集D中随机抽取两个样本，类别标记不一样的概率。值越小，纯度越高。
	- 最终选择使得划分后基尼指数最小的属性作为最优
- 属性a的基尼指数
$$
\operatorname{Gini} \operatorname{index}(D, a)=\sum_{v=1}^V \frac{\left|D^v\right|}{|D|} \operatorname{Gini}\left(D^v\right)
$$

## 4.3 剪枝
- 防止决策树**过拟合**的主要手段。训练过程中，为了尽可能正确分类训练样本呢，可能导致分支过多，造成拟合
- 基本策略：预剪枝和后剪枝

## 4.4 连续与缺失值
### 连续值处理
- 可以采用二分方法，采用区间的中位点作为候选划分点，然后可以公式$(4.2)$

缺失值处理
- 要解决两个问题
1. 如何在属性值缺失的情况下进行划分属性？
2. 给定划分属性，若样本在该属性上值缺失，如何划分？

# 第五章 神经网络
- 标准BP算法和累积BP算法的区别 类似于 SGD和标准梯度下降的区别
- 缓解过拟合的策略
	- 早停
	- 正则化：在误差目标函数中增加一个用于描述网络复杂度的部分
- 跳出局部最小的方法
	- 多组初始化值进行分别训练
	- 模拟退火
	- 随机梯度下降
	- 遗传算法


# 第六章 支持向量机

## 6.1 间隔与支持向量

- 划分超平面: 
$$\boldsymbol{w}^\mathrm {T}\boldsymbol{x} + b = 0\tag{6.1}$$
- 样本点 x 到超平面的距离:
$$
r=\frac{\left|\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right|}{\|\boldsymbol{w}\|}\tag{6.2}
$$
- 距离超平面最近的几个训练样本点，称为**支持向量**，两个**不同类支持向量**到超平面的距离之和，被称为 **间隔** (margin)，为：
$$\gamma=\frac{2}{||\boldsymbol{w}||}\tag{6.4}$$
- 显然我们要求最大化 $\frac{2}{||w||}$，等价于最小化 $||w||^2$，于是，可以改为求：
$$
\begin{array}{ll}
\min _{\boldsymbol{w}, b} & \frac{1}{2}\|\boldsymbol{w}\|^2 \\
\text { s.t. } & y_i\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_i+b\right) \geqslant 1, \quad i=1,2, \ldots, m
\tag{6.6}\end{array}
$$

- **间隔**从表达式上来说仅与 $w$ 有关，但实际上 $b$ 通过约束隐式影响 $w$ 的取值，进而影响间隔

## 6.2 对偶问题

- 希望求解$(6.6)$来得到模型
$$
f(\boldsymbol{x})=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\tag{6.7}
$$

- 对式(6.6)使用拉格朗日乘子法可得到其“对偶问题”(dual problem)，然后求解其对偶问题即可

## 6.3 核函数

- 前面一直都假设训练样本是线性可分的，实际任务中，可能需要将样本从原始空间**映射到更高维的特征空间**，使在高维空间线性可分
- 更多见正文

## 6.4 软间隔与正则化

- 我们一直假定训练样本在样本空间或特征空间中是线性 可分的，即存在一个超平面能将不同类的样本完全划分开，而实际上可能
	- 很难确定合适的核函数
	- 是否是过拟合导致的
- 如何解决？
	- 引入“**软间隔**”，允许某些样本不满足约束

## 6.5 支持向量回归

## 6.6 核方法


# 第七章 贝叶斯分类器

## 7.1 贝叶斯决策论

# 第八章 集成学习

- 基本上是通过多个体学习器和一个结合模块产生输出。可能是同质(homogeneous)的，也可能是异质(heterogeneous)的

## 8.1 个体与集成

- 集成学习：通过构建并结合多个学习器来完成学习任务
- 集成学习通过将多个学习器结合，可以获得比单一学习器显著优越的泛化性能
	- 对“弱学习器”比较明显

- **二分类问题，当超半数基分类器正确，集成分类正确，此时随个体分类器数量增大，继承错误率将指数下降趋向为0。**
	- 但是关键假设是 **个体学习器的误差相互独立**
## 8.2 Boosting - 串行优化

- 个体学习器间存在强依赖关系、必须串行生成的序列化方法
- 先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注, 然后基于调整后的样本分布来训练下一个基学习器
- 主要关注**降低偏差**，基于泛化性能相当弱的学习器构建出很强的集成
- Boosting 从优化角度来看，是用 forward-stagewise 这**种贪心法去最小化损失函数，由于采取的是串行优化的策略，各子模型之间是强相关的，于是子模型之和并不能显著降低方差**。所以说 boosting 主要还是靠降低偏差来提升预测精度
### AdaBoost

 - Adaboosting 算法是一种迭代算法，其核心思想是针对同一个训练集训练不同的弱分类器，然后把这些弱分类器集合起来，构造一个更强的最终分类器。
 - 算法本身是通过改变数据分布来实现的，它根据每次训练集之中的每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权值。总的原则是“增大错误样本的权重，减小正确样本的权重"

## 8.3 Bagging 和 随机森林 - 并行优化

- 个体学习器间不存在强依赖关系、可同时生成的并行化方法；
- 若希望得到泛化性能强的集成，则个体学习器应尽可能独立；虽然无法做到完全独立，可以使其有较大差异；考虑采用相互有交叠的采样子集

### Bagging
- 来源于 Bootstrap AGGregatING
- 可采样出T个含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合
	- 预测输出时，对分类任务简单投票，对回归任务简单平均
- 主要关注**降低方差**，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更为明显.
- Bagging 算法是对训练样本进行采样，产生出若干不同的子集，**再从每个数据子集中训练出一个分类器，取这些分类器的平均，所以是降低模型的方差(variance)。** Bagging 算法和 RandomForest 这种并行算法都有这个效果

### 随机森林 Random Forest
- 传统决策树在选择划分属性时，是在当前节点的**所有属性**中选一个最优属性；而RF中，则是从属性集合中选出一个**包含k个属性的子集**，再从子集中选出一个最优属性用于划分。
	- k=1，就是随机选择一个属性；推荐值为 $k=log_2d$ 
- 比起bagging，多样性不仅来自**样本扰动**，还来自**属性扰动**，使泛化性能进一步增强

# 第九章 聚类

- Unsupervised Learning 中研究做多的任务是聚类；常见的还有密度估计、异常检测等

## 9.1 聚类任务

聚类任务的两个基本问题：性能度量 和 距离计算

## 9.2 性能度量

一种“有效性指标”，一方面可以用来评估好坏，另一方面可以用来作为优化目标。
- 要求 簇内相似度高，簇间相似度低

度量分为两类。
- 外部指标：与某“参考模型”比较
- 内部指标：直接利用聚类的结果
$$
\begin{aligned}
& \left.a=|S S|, \quad S S=\left\{\left(x_i, x_j\right) \mid \lambda_i=\lambda_j, \lambda_i^*=\lambda_j^*, i<j\right)\right\} \\
& \left.b=|S D|, \quad S D=\left\{\left(x_i, x_j\right) \mid \lambda_i=\lambda_j, \lambda_i^* \neq \lambda_j^*, i<j\right)\right\} \\
& \left.c=|D S|, \quad D S=\left\{\left(x_i, x_j\right) \mid \lambda_i \neq \lambda_j, \lambda_i^*=\lambda_j^*, i<j\right)\right\} \\
& \left.d=|D D|, \quad D D=\left\{\left(x_i, x_j\right) \mid \lambda_i \neq \lambda_j, \lambda_i^* \neq \lambda_j^*, i<j\right)\right\}
\end{aligned}
$$
通过聚类得到的划分为$C$，参考模型给出划分为$C^*$
- SS：C中相同，$C^*$中也相同
- SD：C中相同，$C^*$不同
- DS：C中不同，$C^*$相同
- DD：C中不同，$C^*$也不同
- $a+b+c+d=m(m-1)/2$

- Jaccard系数, JC
$$
\mathrm{JC}=\frac{a}{a+b+c}\tag{9.5}
$$
- FM指数, FMI
$$
\mathrm{FMI}=\sqrt{\frac{a}{a+b} \cdot \frac{a}{a+c}}\tag{9.6}
$$
- Rand指数, RI
$$
RI = \frac{2(a+d)}{m(m-1)}
$$
- 这些指标都是$[0,1]$区间，值越大越好

$$
\begin{aligned}
&\begin{aligned}
\operatorname{avg}(C) & =\frac{2}{|C|(|C|-1)} \sum_{1 \leqslant i<j \leqslant|C|} \operatorname{dist}\left(x_i, x_j\right), \\
\operatorname{diam}(C) & =\max _{1 \leqslant i<j \leqslant|C|} \operatorname{dist}\left(x_i, x_j\right), \\
d_{\min }\left(C_i, C_j\right) & =\min _{x_i \in C_i, x_j \in C_j} \operatorname{dist}\left(x_i, x_j\right), \\
d_{\operatorname{cen}}\left(C_i, C_j\right) & =\operatorname{dist}\left(\boldsymbol{\mu}_i, \boldsymbol{\mu}_j\right)
\end{aligned}\\
\end{aligned}
$$

- 其中，$dist(\cdot,\cdot)$用于计算两个样本之间的距离
- $d_{cen}(C_i,C_j)$对应于两个簇之间的中心点距离

- DB指数, DBI
$$
\mathrm{DBI}=\frac{1}{k} \sum_{i=1}^k \max _{j \neq
i}\left(\frac{\operatorname{avg}\left(C_i\right)+\operatorname{avg}\left(C_j\right)}{d_{\operatorname{cen}}\left(\boldsymbol{\mu}_i, \boldsymbol{\mu}_j\right)}\right)\tag{9.12}
$$

- 它计算的是每个**聚类与其最相似聚类之间的平均相似度**。
- DBI值越小，表示聚类结果越好，即聚类内部紧密度高且聚类间分离度好。

- Dunn指数, DI
$$
\mathrm{DI}=\min _{1 \leqslant i \leqslant k}\left\{\min _{j \neq i}\left(\frac{d_{\min }\left(C_i, C_j\right)}{\max _{1 \leqslant l \leqslant k} \operatorname{diam}\left(C_l\right)}\right)\right\} \\
\tag {9.13}
$$
- DI指数衡量的是聚类结果的分离度和紧密度。
- 它计算的是**所有聚类中最小簇间距离与最大簇内距离的比值**。DI值越大，表示聚类结果越好，即聚类间分离度高且聚类内部紧密度好。

## 9.3 距离计算

- 最常用的距离 “闵可夫斯基距离”(Minkowski distance)
$$
\operatorname{dist}_{\mathrm{mk}}\left(x_i, x_j\right)=\left(\sum_{u=1}^n\left|x_{i u}-x_{j u}\right|^p\right)^{\frac{1}{p}}\tag{9.18}
$$
- $p=1$ 时，曼哈顿距离
- $p=2$ 时，欧氏距离

- VDM用于无序属性距离计算，第i个样本簇，取值为a/b，属性u
	- 通过计算不同属性值在不同类别中的分布差异来量化这些属性值之间的差异
$$
\begin{aligned}
&\operatorname{VDM}_p(a, b)=\sum_{i=1}^k\left|\frac{m_{u, a, i}}{m_{u, a}}-\frac{m_{u, b, i}}{m_{u, b}}\right|^p\\
\end{aligned}\tag{9.21}
$$

- 可以用Minkowski距离和VDM距离结合起来，处理混合属性
- 不同属性重要不同，则可以进行加权

## 9.4 原型聚类

“基于原型的聚类”，假设聚类能通过一组原型刻画，初始化原型，然后对原型进行迭代更新求解。

### k均值 k-means

最小化平方误差：
$$
E=\sum_{i=1}^k \sum_{x \in C_i}\left\|x-\mu_i\right\|_2^2\tag{9.24}
$$
其中 $\boldsymbol{\mu}_i=\frac{1}{\left|C_i\right|} \sum_{\boldsymbol{x} \in C_i} \boldsymbol{x}$ 是簇 $C_i$ 的均值向量。
- 直观上反映了对所有簇，簇内样本围绕均值的紧密程度。
- 这是一个NP难问题
- 采用贪心算法，通过迭代求近似解
	1. 选择 k 个样本为初始均值向量
	2. 每次计算样本与所有均值向量的距离，划入最小的类
	3. 计算新均值向量
	4. 不变则结束，变则继续更新


### 学习向量量化 - LVQ

数据带有类别标记，利用样本的监督信息来辅助聚类。
- 每个样本向量由 $n$ 个属性描述
- 目标是学习到一组 $n$ 维原型向量 ${p_1,p_2,\dots,p_q}$ 每个原型向量代表一个聚类簇
	1. 初始化一组原型向量 ${p_1,p_2,\dots,p_q}$ 
	2. 每次计算样本与原型的距离，找到最近的原型 $p_i*$ ，若样本与原型标记相同，则原型向样本移动一部分，否则远离一部分
	3. 反复直到满足特定条件

- Voronoi剖分
	- 学习到一组 $n$ 维原型向量 ${p_1,p_2,\dots,p_q}$ 后，对任意样本，将划入与其最近的簇中，可划分出边界


### 高斯混合聚类


## 9.5 密度聚类

### DBSCAN

一种著名的密度聚类算法。通过“邻域”参数 $(\epsilon, MinPts)$ 刻画，可以得到以下概念：
- $\epsilon$-邻域
- 核心对象
- 密度直达、密度可达、密度相连

基于此，簇定义为 `由密度可达关系导出的最大的密度相连的样本合集`。

如何找到这样的簇？实际上，从 $x$ 密度可达的所有样本即可组成一个簇（同时满足**连接性、最大性**）。
1. 任选一个核心对象为种子，确定簇
2. 直到所有核心对象均被访问
3. 不属于任何簇的被认为是“噪声”或”异常“ (noise, anomaly)

## 9.6 层次聚类

试图在不同层次对数据集进行划分，形成树形聚类结构

### AGNES

1. 将每个样本看作初始聚类簇
2. 每一步找出距离最近的两个聚类簇进行合并
3. 不断重复直到设定值

- 聚类簇距离可以用 $d_{min}$、$d_{max}$、$d_{avg}$ 计算，

# 第十章 降维与度量学习

## 10.1 k近邻学习 - kNN

k-Nearest Neighbor, 常用的监督学习方法。
- 给定测试样本，基于某种距离度量找出训练集中与其最靠近的 $k$ 个训练样本
- 基于这 $k$ 个样本的信息来预测
	- 分类任务中使用“投票法“
	- 回归任务中使用”平均法“


- 这是一种 **”懒惰学习“**，在训练阶段仅仅把样本保存起来，而不进行其他处理；对应的为 **“急切学习”**

## 10.2 低维嵌入

上面的讨论基于一个重要假设：测试样本附近小距离 $\delta$ 内，能够找到一个训练样本。要求训练样本的采样密度足够大，实现“密采样”。
- 实际上在现实任务中很难满足。
- 尤其是当维数很高时

**“维数灾难” (curse of dimensionality)** 指的就是高维情形下，数据样本稀疏、距离计算困难的情景

**“降维” (dimension reduction)** 来缓解。数据样本是高维的，但与学习任务密切相关的可能是一个低维分布，或 **"嵌入" (embedding)** 。

一般来说，若想获得低维子空间，最简单的方法是对原始高维空间进行**线性变换**。

**评估降维效果**通常是比较将为前后学习器的性能。也可以用二维三维可视化技术进行判断。

### 多维缩放 - MDS

要求原始空间中样本之间的**相对距离在低维空间中保持**的降维方法。

## 10.3 主成分分析 - PCA

Principal Component Analysis，主成分分析。

考虑：对于正交属性空间中的样本点，如何用一个超平面（直线的高维推广），对所有样本进行恰当的表达？同时超平面应该具有：
- 最近重构性：样本点到超平面足够近
- 最大可分性：样本点在超平面上投影尽可能分开

算法过程：
1. 所有样本进行中心化
2. 计算样本的协方差矩阵
3. 对协方差矩阵进行特征值分解
4. 取最大的 $d^\prime$ 个特征值所对应的特征向量（也就是投影矩阵）

维数 $d^\prime$ 指定，或用其他学习器验证。也可以设置阈值，





# 第十四章 概率图模型

# 第十五章 规则学习

# 第十六章 强化学习